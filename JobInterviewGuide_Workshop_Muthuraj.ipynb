{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Interview Guide — Personalized Study Notebook for Muthuraj\n",
    "\n",
    "![Image Description](./images/OnlineJobInterview2.png)\n",
    "\n",
    "**Date:** 2026-02-27  \n",
    "**Quiz Score:** 10/15 (67%)  \n",
    "**Model Used:** Claude Opus 4.6  \n",
    "\n",
    "---\n",
    "\n",
    "This notebook was **automatically generated** based on your quiz performance. It focuses on the **5 topics you missed** and provides clear explanations, hands-on code exercises, and practice problems to strengthen those areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You'll Learn\n",
    "\n",
    "1. How to **read residual plots** and understand what different patterns mean for your linear regression model\n",
    "2. The **difference between R² and MSE** — what each measures, their units, and how to interpret them correctly\n",
    "3. How to **compute the sigmoid function by hand** and understand the decision boundary at z=0\n",
    "4. When to use **Manhattan vs. Euclidean distance** in KNN, especially with outliers\n",
    "5. How to **compare models for interpretability** and choose the right one for non-technical stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Vocabulary\n",
    "\n",
    "| Term | Simple Explanation |\n",
    "|------|-------------------|\n",
    "| **Residual** | The difference between what the model predicted and the actual value (error for one data point) |\n",
    "| **R² (R-squared)** | A score from 0 to 1 that tells you what percentage of the data's variation your model explains |\n",
    "| **MSE (Mean Squared Error)** | The average of all squared errors — tells you how far off predictions are (in squared units) |\n",
    "| **RMSE** | Square root of MSE — gives you the error in the same units as your target variable |\n",
    "| **Sigmoid function** | An S-shaped curve that squishes any number into a value between 0 and 1 |\n",
    "| **Decision boundary** | The point where the model switches from predicting one class to the other (default: probability = 0.5) |\n",
    "| **Euclidean distance (L2)** | \"Straight-line\" distance — squares differences, so big outliers dominate |\n",
    "| **Manhattan distance (L1)** | \"City-block\" distance — sums absolute differences, more robust to outliers |\n",
    "| **Interpretability** | How easily a human (like a doctor) can understand *why* a model made a specific prediction |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Results Summary\n",
    "\n",
    "| # | Topic | Your Answer | Correct Answer | Result |\n",
    "|---|-------|-------------|----------------|--------|\n",
    "| 1 | Supervised vs. Unsupervised | B | B | ✅ |\n",
    "| 2 | Dependent vs. Independent Variables | C | C | ✅ |\n",
    "| 3 | Train/Val/Test Split (temporal) | C | C | ✅ |\n",
    "| 4 | **Linear Regression: Residuals** | A | **C** | ❌ |\n",
    "| 5 | **R² vs. MSE** | D | **C** | ❌ |\n",
    "| 6 | **Sigmoid Function** | B | **C** | ❌ |\n",
    "| 7 | Cross-Entropy / Log-Loss | B | B | ✅ |\n",
    "| 8 | KNN: GridSearchCV | C | C | ✅ |\n",
    "| 9 | **KNN: Distance Metrics** | E | **B** | ❌ |\n",
    "| 10 | Decision Trees: Leaf Nodes | C | C | ✅ |\n",
    "| 11 | Decision Trees: Overfitting | B | B | ✅ |\n",
    "| 12 | Data Leakage / Pipelines | C | C | ✅ |\n",
    "| 13 | Imbalanced Data (Behavioral) | B | B | ✅ |\n",
    "| 14 | **Model Interpretability Tradeoffs** | A | **C** | ❌ |\n",
    "| 15 | Putting It All Together | C | C | ✅ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings to keep the output clean for reading\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries we'll use throughout this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a clean visual style for all our plots\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# random_state=42 ensures we get the same results every time we run this\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"All libraries loaded successfully!\")\n",
    "print(\"Let's strengthen the 5 topics you missed on the quiz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Linear Regression — Reading Residual Plots\n",
    "\n",
    "**You missed this on Quiz Question 4.** You answered that randomly scattered residuals mean the model is overfitting. Actually, random scatter is a **good sign** — it means the linear assumption holds.\n",
    "\n",
    "### The Big Idea\n",
    "\n",
    "Think of residuals like a teacher grading exam answers:\n",
    "- The **residual** for each data point = `actual value - predicted value` (the \"error\")\n",
    "- If you plot all the errors and they look **random** (no patterns), it means the model captured all the real signal and only random noise is left — that's **good**!\n",
    "- If you see a **curve or funnel** in the errors, the model is missing something — that's **bad**.\n",
    "\n",
    "### What Different Residual Patterns Mean\n",
    "\n",
    "| Pattern You See | What It Means | What To Do |\n",
    "|----------------|---------------|------------|\n",
    "| Random scatter around zero | ✅ Linear assumption is valid | Nothing — your model is appropriate |\n",
    "| U-shaped or curved pattern | ❌ Non-linear relationship | Try polynomial features or log transform (linearization) |\n",
    "| Funnel shape (wider on one side) | ❌ Heteroscedasticity (unequal variance) | Try log-transforming the target variable |\n",
    "| Residuals all near zero (training) but wild (testing) | ❌ Overfitting | Simplify the model or get more data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# --- Generate 3 different datasets to show 3 residual patterns ---\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "x = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "\n",
    "# Dataset 1: True linear relationship (good residuals expected)\n",
    "y_linear = 3 * x.ravel() + 7 + np.random.normal(0, 3, 200)\n",
    "\n",
    "# Dataset 2: Quadratic relationship (curved residuals expected)\n",
    "y_curved = 0.5 * x.ravel()**2 + np.random.normal(0, 2, 200)\n",
    "\n",
    "# Dataset 3: Heteroscedastic relationship (funnel residuals expected)\n",
    "y_funnel = 2 * x.ravel() + np.random.normal(0, 1, 200) * x.ravel()\n",
    "\n",
    "print(\"Three synthetic datasets created:\")\n",
    "print(\"1) Linear relationship (y = 3x + 7 + noise)\")\n",
    "print(\"2) Quadratic relationship (y = 0.5x² + noise)\")\n",
    "print(\"3) Heteroscedastic relationship (noise grows with x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple linear regression to ALL three datasets\n",
    "# (even though linear regression is only appropriate for dataset 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "datasets = [\n",
    "    (y_linear, \"Good: Random Scatter\\n(Linear assumption is valid)\"),\n",
    "    (y_curved, \"Bad: U-Shaped Curve\\n(Non-linear relationship missed)\"),\n",
    "    (y_funnel, \"Bad: Funnel Shape\\n(Variance increases with predictions)\")\n",
    "]\n",
    "\n",
    "for idx, (y_data, title) in enumerate(datasets):\n",
    "    # Train a linear regression model\n",
    "    model = LinearRegression().fit(x, y_data)\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    # Calculate residuals = actual - predicted\n",
    "    residuals = y_data - y_pred\n",
    "    \n",
    "    # Plot residuals vs predicted values\n",
    "    axes[idx].scatter(y_pred, residuals, alpha=0.5, s=20)\n",
    "    axes[idx].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[idx].set_xlabel('Predicted Values', fontsize=12)\n",
    "    axes[idx].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "    axes[idx].set_title(title, fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left plot:   Random scatter around zero = GOOD. The model works well.\")\n",
    "print(\"Middle plot: Curved pattern = BAD. A straight line can't capture a curve.\")\n",
    "print(\"Right plot:  Funnel pattern = BAD. Errors grow as predictions get larger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see here?**\n",
    "\n",
    "- **Left plot (random scatter):** The residuals bounce randomly above and below zero with no visible pattern. This is exactly what we WANT. It means the linear model captured the real relationship, and only unpredictable noise remains.\n",
    "- **Middle plot (U-shape):** The residuals form a clear curve — positive on the edges, negative in the middle. This screams \"the data has a curve, but you forced a straight line!\" The fix: use polynomial features or a log transform (linearization).\n",
    "- **Right plot (funnel):** The residuals fan out as predictions increase. This means the model's errors are larger for bigger values — a violation of the \"equal variance\" assumption.\n",
    "\n",
    "**Key takeaway:** Random scatter = good model fit. Any visible pattern = the model is missing something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 1: Residual Diagnosis Practice ---\n",
    "# Let's fix the curved residuals from dataset 2 using polynomial features\n",
    "\n",
    "# Step 1: Create polynomial features (degree 2 = adds x² as a feature)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "x_poly = poly.fit_transform(x)\n",
    "\n",
    "print(f\"Original features shape: {x.shape}  (just x)\")\n",
    "print(f\"Polynomial features shape: {x_poly.shape}  (x and x²)\")\n",
    "\n",
    "# Step 2: Fit linear regression on the polynomial features\n",
    "model_poly = LinearRegression().fit(x_poly, y_curved)\n",
    "y_pred_poly = model_poly.predict(x_poly)\n",
    "residuals_poly = y_curved - y_pred_poly\n",
    "\n",
    "# Step 3: Compare residual plots — before and after linearization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Before: linear model on curved data\n",
    "model_bad = LinearRegression().fit(x, y_curved)\n",
    "residuals_bad = y_curved - model_bad.predict(x)\n",
    "axes[0].scatter(model_bad.predict(x), residuals_bad, alpha=0.5, s=20)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_title('Before: Linear Model on Curved Data', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[0].set_ylabel('Residuals', fontsize=12)\n",
    "\n",
    "# After: polynomial model on curved data\n",
    "axes[1].scatter(y_pred_poly, residuals_poly, alpha=0.5, s=20)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('After: Polynomial Model (Linearization Fix)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare R² scores\n",
    "r2_before = r2_score(y_curved, model_bad.predict(x))\n",
    "r2_after = r2_score(y_curved, y_pred_poly)\n",
    "print(f\"\\nR² before linearization: {r2_before:.4f}\")\n",
    "print(f\"R² after linearization:  {r2_after:.4f}\")\n",
    "print(f\"\\nThe polynomial model captures the curve and the residuals become random!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**\n",
    "\n",
    "- The left plot shows the **U-shaped pattern** we saw before — the linear model can't handle the curve.\n",
    "- The right plot shows **random scatter** after adding polynomial features — the curve has been captured!\n",
    "- R² jumps dramatically because the model now fits the data's true shape.\n",
    "- This technique — transforming features so a linear model can handle non-linear relationships — is called **linearization**.\n",
    "\n",
    "Now that we understand residuals, let's move on to the metrics that summarize model quality numerically...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regression Metrics — R² vs. MSE\n",
    "\n",
    "**You missed this on Quiz Question 5.** You said R² and MSE measure the same thing on different scales. They actually measure **different things entirely**.\n",
    "\n",
    "### The Analogy\n",
    "\n",
    "Think of it like grading a student's exam:\n",
    "- **MSE** is like counting the total number of **points lost** across all questions. It tells you \"how many points did they miss?\" — an absolute measure in the same units as the exam.\n",
    "- **R²** is like asking \"what **percentage** of the material did they understand?\" — a relative measure between 0% and 100%.\n",
    "\n",
    "A student could lose 20 points (MSE) on an easy 50-point test (R² = 0.60) or lose 20 points on a hard 200-point test (R² = 0.90). Same absolute error, very different relative performance.\n",
    "\n",
    "### Side-by-Side Comparison\n",
    "\n",
    "| Feature | MSE | R² |\n",
    "|---------|-----|----|\n",
    "| **What it measures** | Average squared prediction error | Proportion of variance explained |\n",
    "| **Units** | Squared units of target (e.g., Amps²) | Unitless (0 to 1) |\n",
    "| **Perfect score** | 0 (no errors) | 1.0 (explains everything) |\n",
    "| **Worst score** | ∞ (huge errors) | 0 or negative (worse than guessing the mean) |\n",
    "| **Can compare across datasets?** | No (depends on target scale) | Yes (always 0-1 scale) |\n",
    "| **Formula** | (1/n) × Σ(actual - predicted)² | 1 - (Σ(actual - predicted)² / Σ(actual - mean)²) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Demo: R² and MSE tell you DIFFERENT things ---\n",
    "\n",
    "# Create two datasets with very different scales\n",
    "# Dataset A: small target values (e.g., temperature in Celsius)\n",
    "X_a, y_a = make_regression(n_samples=200, n_features=1, noise=5, random_state=RANDOM_STATE)\n",
    "\n",
    "# Dataset B: large target values (e.g., house prices in thousands)\n",
    "X_b, y_b = make_regression(n_samples=200, n_features=1, noise=50, random_state=RANDOM_STATE)\n",
    "y_b = y_b * 10  # Scale up to simulate larger target values\n",
    "\n",
    "# Train linear regression on both\n",
    "model_a = LinearRegression().fit(X_a, y_a)\n",
    "model_b = LinearRegression().fit(X_b, y_b)\n",
    "\n",
    "pred_a = model_a.predict(X_a)\n",
    "pred_b = model_b.predict(X_b)\n",
    "\n",
    "# Compute metrics\n",
    "results = pd.DataFrame({\n",
    "    'Dataset': ['A (small scale)', 'B (large scale)'],\n",
    "    'MSE': [mean_squared_error(y_a, pred_a), mean_squared_error(y_b, pred_b)],\n",
    "    'RMSE': [np.sqrt(mean_squared_error(y_a, pred_a)), np.sqrt(mean_squared_error(y_b, pred_b))],\n",
    "    'R²': [r2_score(y_a, pred_a), r2_score(y_b, pred_b)]\n",
    "})\n",
    "\n",
    "print(\"Comparing MSE and R² across two datasets with different scales:\\n\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "print(\"\\n--- Observations ---\")\n",
    "print(\"MSE is MUCH larger for Dataset B — but that's just because the target values are bigger.\")\n",
    "print(\"R² is similar for both — both models explain a similar proportion of variance.\")\n",
    "print(\"\\nThis is why R² is useful for comparing models across different datasets,\")\n",
    "print(\"while MSE tells you the actual error magnitude within one dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 2: Interpret R² values correctly ---\n",
    "# This is the exact scenario from Quiz Question 5\n",
    "\n",
    "# From your Robot Predictive Maintenance workshop:\n",
    "r_squared = 0.0046\n",
    "mse = 70.99\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QUIZ QUESTION 5 — Let's break it down\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nR² = {r_squared}\")\n",
    "print(f\"MSE = {mse}\")\n",
    "\n",
    "# Common mistake: misreading the decimal\n",
    "print(f\"\\n❌ WRONG interpretation: 'R² = 0.0046 means 46% variance explained'\")\n",
    "print(f\"   0.0046 is NOT 46%. To convert to percentage: 0.0046 × 100 = 0.46%\")\n",
    "\n",
    "print(f\"\\n✅ CORRECT interpretation: 'R² = 0.0046 means only 0.46% variance explained'\")\n",
    "print(f\"   The model explains almost NOTHING. It's barely better than predicting the mean.\")\n",
    "\n",
    "# RMSE gives us the error in original units\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"\\nRMSE = √MSE = √{mse} = {rmse:.2f} Amps\")\n",
    "print(f\"This means predictions are off by about {rmse:.2f} Amps on average.\")\n",
    "\n",
    "print(f\"\\n--- Key Distinction ---\")\n",
    "print(f\"MSE = {mse} Amps² → tells you HOW FAR OFF predictions are (absolute error)\")\n",
    "print(f\"R²  = {r_squared} → tells you HOW MUCH VARIANCE is explained (relative fit quality)\")\n",
    "print(f\"They measure DIFFERENT things. MSE could be low while R² is also low,\")\n",
    "print(f\"if the target variable itself doesn't vary much.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**\n",
    "\n",
    "- **R² = 0.0046** does NOT mean 46%. It means **0.46%** — watch that decimal! Always multiply by 100 to get the percentage.\n",
    "- **MSE = 70.99** is in **squared** units (Amps²). Take the square root to get RMSE = 8.43 Amps — that's the average error in the original units.\n",
    "- They measure **different things**: MSE = \"how far off?\" (absolute), R² = \"how much explained?\" (relative).\n",
    "- A model can have a low MSE but still a low R² if the target variable barely varies.\n",
    "\n",
    "Now that we have a solid grasp on regression metrics, let's switch gears to classification and the sigmoid function...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Logistic Regression — The Sigmoid Function\n",
    "\n",
    "**You missed this on Quiz Question 6.** You said σ(0) = 1. Actually, σ(0) = **0.5** — the decision boundary.\n",
    "\n",
    "### The Analogy\n",
    "\n",
    "Think of the sigmoid function like a **dimmer switch** for a light:\n",
    "- Turn the dial far to the left (z very negative) → light is OFF (probability ≈ 0)\n",
    "- Turn it far to the right (z very positive) → light is fully ON (probability ≈ 1)\n",
    "- Dial is in the middle (z = 0) → light is at **exactly 50% brightness** (probability = 0.5)\n",
    "\n",
    "The sigmoid function takes ANY number and squishes it into the range (0, 1) — perfect for probabilities!\n",
    "\n",
    "### The Formula\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where:\n",
    "- **z** = the model's raw score (intercept + slope × feature value)\n",
    "- **e** ≈ 2.718 (Euler's number — a mathematical constant)\n",
    "- The output is always between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Let's compute sigmoid by hand for several z values ---\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function: squishes any number into the range (0, 1).\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "# Compute sigmoid for key z values\n",
    "z_values = [-10, -5, -2, -1, 0, 1, 2, 5, 10]\n",
    "\n",
    "print(\"Let's compute σ(z) step by step:\\n\")\n",
    "print(f\"{'z':>5}  |  {'e^(-z)':>10}  |  {'1 + e^(-z)':>12}  |  {'σ(z)':>8}  |  Interpretation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for z in z_values:\n",
    "    exp_neg_z = np.exp(-z)\n",
    "    denominator = 1 + exp_neg_z\n",
    "    sigma = sigmoid(z)\n",
    "    \n",
    "    # Interpretation\n",
    "    if sigma < 0.3:\n",
    "        interp = \"Confident class 0\"\n",
    "    elif sigma < 0.5:\n",
    "        interp = \"Leaning class 0\"\n",
    "    elif sigma == 0.5:\n",
    "        interp = \"<-- DECISION BOUNDARY (50/50)\"\n",
    "    elif sigma < 0.7:\n",
    "        interp = \"Leaning class 1\"\n",
    "    else:\n",
    "        interp = \"Confident class 1\"\n",
    "    \n",
    "    print(f\"{z:>5}  |  {exp_neg_z:>10.4f}  |  {denominator:>12.4f}  |  {sigma:>8.4f}  |  {interp}\")\n",
    "\n",
    "print(\"\\n--- The Quiz Question Walkthrough ---\")\n",
    "print(f\"σ(0) = 1 / (1 + e^(-0)) = 1 / (1 + e^0) = 1 / (1 + 1) = 1/2 = 0.5\")\n",
    "print(f\"At z=0, the model is MAXIMALLY UNCERTAIN — exactly 50/50.\")\n",
    "print(f\"This is why 0.5 is the default decision threshold!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the S-curve with key points marked ---\n",
    "\n",
    "z_range = np.linspace(-10, 10, 300)\n",
    "sigma_values = sigmoid(z_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the sigmoid curve\n",
    "ax.plot(z_range, sigma_values, linewidth=3, label='σ(z) = 1 / (1 + e⁻ᶻ)')\n",
    "\n",
    "# Mark the decision boundary at z=0, σ=0.5\n",
    "ax.scatter([0], [0.5], color='red', s=150, zorder=5, label='z=0 → σ=0.5 (Decision Boundary)')\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Threshold = 0.5')\n",
    "ax.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "# Mark a few other key points\n",
    "key_points = [(-5, sigmoid(-5)), (5, sigmoid(5))]\n",
    "for z_pt, s_pt in key_points:\n",
    "    ax.scatter([z_pt], [s_pt], color='green', s=100, zorder=5)\n",
    "    ax.annotate(f'z={z_pt}, σ={s_pt:.3f}', xy=(z_pt, s_pt), \n",
    "                xytext=(z_pt + 1.5, s_pt - 0.1), fontsize=10,\n",
    "                arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "# Labels and regions\n",
    "ax.fill_between(z_range, 0, sigma_values, where=(sigma_values < 0.5), alpha=0.1, color='blue', label='Predict Class 0')\n",
    "ax.fill_between(z_range, sigma_values, 1, where=(sigma_values >= 0.5), alpha=0.1, color='orange', label='Predict Class 1')\n",
    "\n",
    "ax.set_xlabel('z (raw model output)', fontsize=13)\n",
    "ax.set_ylabel('σ(z) (probability)', fontsize=13)\n",
    "ax.set_title('The Sigmoid Function — The Heart of Logistic Regression', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The S-curve maps ANY number to a probability between 0 and 1.\")\n",
    "print(\"z < 0 → σ < 0.5 → predict class 0\")\n",
    "print(\"z = 0 → σ = 0.5 → maximally uncertain (decision boundary)\")\n",
    "print(\"z > 0 → σ > 0.5 → predict class 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 3: Sigmoid + Logistic Regression on real data ---\n",
    "# Let's see how intercept and slope connect to the sigmoid\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Simple dataset: hours studied vs pass/fail (from your workshop)\n",
    "hours_studied = np.array([1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10]).reshape(-1, 1)\n",
    "passed = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Train logistic regression\n",
    "log_reg = LogisticRegression(random_state=RANDOM_STATE)\n",
    "log_reg.fit(hours_studied, passed)\n",
    "\n",
    "intercept = log_reg.intercept_[0]\n",
    "slope = log_reg.coef_[0][0]\n",
    "\n",
    "print(f\"Trained Logistic Regression Model:\")\n",
    "print(f\"  Intercept (β₀) = {intercept:.4f}\")\n",
    "print(f\"  Slope (β₁)     = {slope:.4f}\")\n",
    "print(f\"\\nThe model computes: z = {intercept:.4f} + {slope:.4f} × hours\")\n",
    "print(f\"Then feeds z into the sigmoid: probability = σ(z)\")\n",
    "\n",
    "# Find the decision boundary: where σ(z) = 0.5, which means z = 0\n",
    "# z = intercept + slope * hours = 0  →  hours = -intercept / slope\n",
    "boundary_hours = -intercept / slope\n",
    "print(f\"\\nDecision boundary at: hours = -{intercept:.4f} / {slope:.4f} = {boundary_hours:.2f} hours\")\n",
    "print(f\"Students who study more than {boundary_hours:.1f} hours are predicted to PASS.\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_plot = np.linspace(0, 11, 300).reshape(-1, 1)\n",
    "probabilities = log_reg.predict_proba(x_plot)[:, 1]\n",
    "\n",
    "ax.plot(x_plot, probabilities, linewidth=3, label='P(Pass) = σ(β₀ + β₁ × hours)')\n",
    "ax.scatter(hours_studied, passed, color='red', s=80, zorder=5, label='Actual data (0=Fail, 1=Pass)')\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Decision threshold (0.5)')\n",
    "ax.axvline(x=boundary_hours, color='green', linestyle=':', alpha=0.7, label=f'Decision boundary ({boundary_hours:.1f} hrs)')\n",
    "\n",
    "ax.set_xlabel('Hours Studied', fontsize=13)\n",
    "ax.set_ylabel('Probability of Passing', fontsize=13)\n",
    "ax.set_title('Logistic Regression: Hours Studied vs. Pass Probability', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**\n",
    "\n",
    "- The **intercept** (β₀) shifts the S-curve left or right — it controls where the decision boundary falls.\n",
    "- The **slope** (β₁) controls how steep the curve is — a steeper curve means the model is more decisive (transitions quickly from 0 to 1).\n",
    "- The **decision boundary** is where z = 0 (so σ = 0.5). For a single feature: `boundary = -intercept / slope`.\n",
    "- At z = 0, the sigmoid outputs **exactly 0.5** — this is the mathematical fact you missed on the quiz.\n",
    "\n",
    "Now let's tackle the KNN distance metric question...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: KNN — Distance Metrics and Outliers\n",
    "\n",
    "**You missed this on Quiz Question 9.** You chose to remove outliers rather than switching the distance metric. While outlier removal can sometimes help, in medical data extreme values are often **clinically meaningful** (e.g., a very high glucose reading is real, not noise). The better approach is to use a **distance metric that is robust to outliers**.\n",
    "\n",
    "### The Analogy\n",
    "\n",
    "Imagine you're comparing two students based on their grades in Math and English:\n",
    "- **Student A:** Math = 85, English = 82\n",
    "- **Student B:** Math = 88, English = 80\n",
    "- **Student C:** Math = 85, English = 200 (data entry error? or a genius?)\n",
    "\n",
    "**Euclidean distance** (L2) between A and C:\n",
    "- √((85-85)² + (82-200)²) = √(0 + 13924) = **118.0**\n",
    "- The outlier in English **completely dominates** because squaring amplifies big differences.\n",
    "\n",
    "**Manhattan distance** (L1) between A and C:\n",
    "- |85-85| + |82-200| = 0 + 118 = **118**\n",
    "- Same number here, but with multiple features, Manhattan spreads the influence more evenly.\n",
    "\n",
    "### When Does It Really Matter?\n",
    "\n",
    "With more features, the difference becomes dramatic. Let's see it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean, cityblock\n",
    "\n",
    "# --- Demo: How outliers affect Euclidean vs Manhattan distance ---\n",
    "\n",
    "# Two patients with 5 medical features\n",
    "patient_a = np.array([120, 25, 72, 30, 80])     # glucose, BMI, BP, skin, insulin\n",
    "patient_b = np.array([125, 27, 75, 32, 85])     # similar to patient A\n",
    "patient_c = np.array([122, 26, 73, 31, 500])    # similar, but EXTREME insulin (outlier)\n",
    "\n",
    "print(\"Patient features: [Glucose, BMI, BloodPressure, SkinThickness, Insulin]\")\n",
    "print(f\"Patient A: {patient_a}\")\n",
    "print(f\"Patient B: {patient_b}  (similar to A in all features)\")\n",
    "print(f\"Patient C: {patient_c}  (similar to A, but extreme insulin = 500)\")\n",
    "\n",
    "print(\"\\n--- Distances from Patient A ---\")\n",
    "print(f\"{'':>25} {'Euclidean (L2)':>15} {'Manhattan (L1)':>15}\")\n",
    "print(f\"{'To Patient B (normal)':>25} {euclidean(patient_a, patient_b):>15.2f} {cityblock(patient_a, patient_b):>15.2f}\")\n",
    "print(f\"{'To Patient C (outlier)':>25} {euclidean(patient_a, patient_c):>15.2f} {cityblock(patient_a, patient_c):>15.2f}\")\n",
    "\n",
    "# Show the ratio of how much the outlier inflates the distance\n",
    "euc_ratio = euclidean(patient_a, patient_c) / euclidean(patient_a, patient_b)\n",
    "man_ratio = cityblock(patient_a, patient_c) / cityblock(patient_a, patient_b)\n",
    "\n",
    "print(f\"\\n--- Inflation Ratio (outlier distance / normal distance) ---\")\n",
    "print(f\"Euclidean: Patient C appears {euc_ratio:.1f}x farther than Patient B\")\n",
    "print(f\"Manhattan: Patient C appears {man_ratio:.1f}x farther than Patient B\")\n",
    "print(f\"\\nEuclidean exaggerates the outlier's effect by {euc_ratio/man_ratio:.1f}x more than Manhattan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Why squaring matters: breaking down the distance components ---\n",
    "\n",
    "feature_names = ['Glucose', 'BMI', 'BloodPressure', 'SkinThickness', 'Insulin']\n",
    "diffs = patient_c - patient_a  # raw differences\n",
    "\n",
    "breakdown = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Patient A': patient_a,\n",
    "    'Patient C': patient_c,\n",
    "    'Difference': diffs,\n",
    "    'L1 Contribution (|diff|)': np.abs(diffs),\n",
    "    'L2 Contribution (diff²)': diffs**2\n",
    "})\n",
    "\n",
    "print(\"Feature-by-feature distance breakdown (Patient A vs Patient C):\\n\")\n",
    "print(breakdown.to_string(index=False))\n",
    "\n",
    "print(f\"\\nL1 total = sum of |diff| = {np.sum(np.abs(diffs))}\")\n",
    "print(f\"L2 total = sqrt(sum of diff²) = sqrt({np.sum(diffs**2)}) = {np.sqrt(np.sum(diffs**2)):.2f}\")\n",
    "\n",
    "# What percentage of the total distance comes from Insulin?\n",
    "l1_insulin_pct = np.abs(diffs[-1]) / np.sum(np.abs(diffs)) * 100\n",
    "l2_insulin_pct = diffs[-1]**2 / np.sum(diffs**2) * 100\n",
    "\n",
    "print(f\"\\nInsulin's share of total distance:\")\n",
    "print(f\"  Manhattan (L1): {l1_insulin_pct:.1f}% — Insulin matters, but other features still count\")\n",
    "print(f\"  Euclidean (L2): {l2_insulin_pct:.1f}% — Insulin DOMINATES everything else\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 4: Compare KNN with Euclidean vs Manhattan on data with outliers ---\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create a dataset with some outlier features\n",
    "np.random.seed(RANDOM_STATE)\n",
    "X_clean, y_clean = make_classification(\n",
    "    n_samples=500, n_features=5, n_informative=3,\n",
    "    n_redundant=0, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Inject outliers into one feature (simulating extreme insulin readings)\n",
    "X_outlier = X_clean.copy()\n",
    "outlier_mask = np.random.random(500) < 0.05  # 5% of samples get outliers\n",
    "X_outlier[outlier_mask, 0] = X_outlier[outlier_mask, 0] * 20  # extreme values\n",
    "\n",
    "print(f\"Dataset: {X_outlier.shape[0]} samples, {X_outlier.shape[1]} features\")\n",
    "print(f\"Outliers injected into feature 0 for {outlier_mask.sum()} samples ({outlier_mask.mean()*100:.0f}%)\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_outlier, y_clean, test_size=0.20, stratify=y_clean, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Compare Euclidean vs Manhattan with a Pipeline (prevents data leakage!)\n",
    "metrics_to_test = ['euclidean', 'manhattan']\n",
    "comparison_results = []\n",
    "\n",
    "for metric in metrics_to_test:\n",
    "    # Pipeline ensures scaler is fit only on training data\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5, metric=metric, weights='distance'))\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Distance Metric': metric.capitalize(),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nKNN Performance with Outlier-Contaminated Data:\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nManhattan is more robust to the outliers because it doesn't square the differences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**\n",
    "\n",
    "- **Euclidean (L2)** squares differences → outliers get amplified → one extreme feature can dominate the entire distance calculation.\n",
    "- **Manhattan (L1)** uses absolute differences → outliers still contribute, but proportionally, not quadratically.\n",
    "- **Removing outliers** (your quiz answer) isn't always appropriate — in medical data, extreme values are often real and clinically important. The better approach is to choose a **robust distance metric**.\n",
    "- Always remember: **scale your features first** (StandardScaler) and put it inside a **Pipeline** to prevent data leakage.\n",
    "\n",
    "One more topic to cover — model interpretability...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Model Interpretability — Choosing the Right Model for the Audience\n",
    "\n",
    "**You missed this on Quiz Question 14.** You chose KNN for its non-parametric nature, but the question asked for **interpretability** — how easily a non-technical person can understand the model's reasoning.\n",
    "\n",
    "### The Analogy\n",
    "\n",
    "Imagine explaining to a doctor *why* a patient was flagged as high-risk:\n",
    "\n",
    "- **Decision Tree says:** \"Because the patient's glucose is above 140 AND their BMI is above 35. Here's a flowchart showing exactly how I decided.\" → The doctor can **read it, verify it, and trust it**.\n",
    "\n",
    "- **Logistic Regression says:** \"The patient's risk score = -4.2 + 0.03 × glucose + 0.08 × BMI + ... Each unit of BMI increases the log-odds by 0.08.\" → A data-savvy doctor can understand this, but **log-odds aren't intuitive** for most people.\n",
    "\n",
    "- **KNN says:** \"These 5 other patients in the database were similar to yours, and 4 of them were high-risk.\" → But the doctor asks: *\"Similar how? Which features mattered? Can I see the rule?\"* → **KNN has no rule to show.**\n",
    "\n",
    "### Interpretability Comparison\n",
    "\n",
    "| Model | Interpretability | What It Can Explain | Best For |\n",
    "|-------|-----------------|--------------------|---------|\n",
    "| **Decision Tree** | Highest | Visual if/then flowchart, exact rules | Non-technical stakeholders (doctors, managers) |\n",
    "| **Logistic Regression** | High | Feature coefficients, odds ratios | Technical audiences who understand statistics |\n",
    "| **KNN** | Low | \"Similar neighbors voted\" — no explicit rules | When interpretability isn't the priority |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 5: See the interpretability difference yourself ---\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load a real medical dataset\n",
    "data = load_breast_cancer()\n",
    "X_cancer = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y_cancer = data.target  # 0 = malignant, 1 = benign\n",
    "\n",
    "# Use just 4 key features to keep it readable\n",
    "selected_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']\n",
    "X_selected = X_cancer[selected_features]\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_selected, y_cancer, test_size=0.20, stratify=y_cancer, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Breast Cancer dataset: {X_selected.shape[0]} samples, {len(selected_features)} features\")\n",
    "print(f\"Features: {selected_features}\")\n",
    "print(f\"Target: 0 = Malignant, 1 = Benign\")\n",
    "print(f\"Train: {len(X_train_c)}, Test: {len(X_test_c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 1: Decision Tree (MOST interpretable) ---\n",
    "\n",
    "# max_depth=3 keeps the tree small enough for a human to read\n",
    "dt_model = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)\n",
    "dt_model.fit(X_train_c, y_train_c)\n",
    "\n",
    "dt_acc = accuracy_score(y_test_c, dt_model.predict(X_test_c))\n",
    "print(f\"Decision Tree Test Accuracy: {dt_acc:.4f}\")\n",
    "\n",
    "# Visualize the tree — THIS is what makes it interpretable\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    dt_model,\n",
    "    feature_names=selected_features,\n",
    "    class_names=['Malignant', 'Benign'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=11,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Decision Tree — A Doctor Can Read This Like a Flowchart', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"A doctor can follow this tree from top to bottom:\")\n",
    "print(\"  'If mean perimeter <= X, go left. Otherwise, go right.'\")\n",
    "print(\"  At each leaf: 'Predict Malignant' or 'Predict Benign'.\")\n",
    "print(\"  No math degree required!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 2: Logistic Regression (interpretable to technical people) ---\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "log_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000))\n",
    "])\n",
    "log_pipe.fit(X_train_c, y_train_c)\n",
    "\n",
    "log_acc = accuracy_score(y_test_c, log_pipe.predict(X_test_c))\n",
    "print(f\"Logistic Regression Test Accuracy: {log_acc:.4f}\\n\")\n",
    "\n",
    "# Show the coefficients — this IS the interpretability\n",
    "coefs = log_pipe.named_steps['clf'].coef_[0]\n",
    "intercept_lr = log_pipe.named_steps['clf'].intercept_[0]\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Coefficient': coefs,\n",
    "    'Interpretation': [\n",
    "        'Higher radius → more likely malignant' if c < 0 else 'Higher radius → more likely benign'\n",
    "        for c in coefs\n",
    "    ]\n",
    "})\n",
    "coef_df = coef_df.sort_values('Coefficient')\n",
    "\n",
    "print(f\"Intercept: {intercept_lr:.4f}\\n\")\n",
    "print(\"Feature Coefficients (what each feature contributes to the prediction):\")\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nA data-savvy doctor could say: 'Larger mean area pushes toward benign,\")\n",
    "print(\"while larger mean perimeter pushes toward malignant.'\")\n",
    "print(\"But explaining 'log-odds' to a non-technical person is harder than a flowchart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 3: KNN (LEAST interpretable) ---\n",
    "\n",
    "knn_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=5, metric='manhattan', weights='distance'))\n",
    "])\n",
    "knn_pipe.fit(X_train_c, y_train_c)\n",
    "\n",
    "knn_acc = accuracy_score(y_test_c, knn_pipe.predict(X_test_c))\n",
    "print(f\"KNN Test Accuracy: {knn_acc:.4f}\\n\")\n",
    "\n",
    "# KNN can tell you WHO the neighbors are, but not WHY the decision was made\n",
    "sample = X_test_c.iloc[[0]]\n",
    "distances, indices = knn_pipe.named_steps['clf'].kneighbors(\n",
    "    knn_pipe.named_steps['scaler'].transform(sample)\n",
    ")\n",
    "\n",
    "print(f\"For test sample 0, KNN found these 5 nearest neighbors:\")\n",
    "print(f\"  Neighbor indices in training set: {indices[0]}\")\n",
    "print(f\"  Distances: {distances[0].round(3)}\")\n",
    "print(f\"  Their labels: {y_train_c[indices[0]]}\")\n",
    "print(f\"  Prediction: {'Benign' if knn_pipe.predict(sample)[0] == 1 else 'Malignant'}\")\n",
    "\n",
    "print(\"\\nIf a doctor asks 'WHY is this malignant?', KNN can only say:\")\n",
    "print(\"  'Because 4 out of 5 similar patients were malignant.'\")\n",
    "print(\"  Doctor: 'Similar HOW? Which features mattered most?'\")\n",
    "print(\"  KNN: '... I don't have explicit rules to show you.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Comparison Table ---\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Decision Tree (depth=3)', 'Logistic Regression', 'KNN (k=5, Manhattan)'],\n",
    "    'Test Accuracy': [dt_acc, log_acc, knn_acc],\n",
    "    'Interpretability': ['Highest — visual flowchart', 'High — feature coefficients', 'Low — no explicit rules'],\n",
    "    'Best Audience': ['Non-technical (doctors, managers)', 'Technical (data-savvy)', 'When interpretability not needed']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"MODEL COMPARISON — Accuracy vs. Interpretability\")\n",
    "print(\"=\" * 90)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\nKey lesson: The 'best' model depends on your AUDIENCE, not just accuracy.\")\n",
    "print(\"When a doctor needs to understand and trust the model → Decision Tree wins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**\n",
    "\n",
    "- **Interpretability is a feature**, not an afterthought. In healthcare, finance, and regulated industries, being able to explain *why* a model made a decision is often **legally required**.\n",
    "- Decision Trees are the gold standard for interpretability — they produce human-readable if/then rules.\n",
    "- KNN may perform well, but it's a **\"black box\" for explanations** — it can't articulate rules.\n",
    "- The right model choice depends on who will use the results, not just which has the highest accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Takeaways\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "| Part | Topic You Missed | Key Lesson Learned |\n",
    "|------|-----------------|--------------------|\n",
    "| 1 | Residual Plot Interpretation | Random scatter = GOOD (linear assumption holds). Patterns = BAD (model is missing something). |\n",
    "| 2 | R² vs. MSE | They measure DIFFERENT things: MSE = absolute error (in squared units), R² = proportion of variance explained (unitless, 0-1). |\n",
    "| 3 | Sigmoid Function at z=0 | σ(0) = 0.5 always. This is the decision boundary — the model is maximally uncertain. |\n",
    "| 4 | Distance Metrics with Outliers | Manhattan (L1) is more robust to outliers because it doesn't square differences. Don't just remove outliers — they may be meaningful. |\n",
    "| 5 | Model Interpretability | Decision Trees > Logistic Regression > KNN for interpretability. Choose based on your audience. |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Residual plots are diagnostic tools** — learn to read them like a doctor reads an X-ray. Random = healthy, patterns = problems.\n",
    "2. **R² and MSE are complementary, not interchangeable.** Always report both. And watch those decimals — 0.0046 is 0.46%, not 46%!\n",
    "3. **The sigmoid function always outputs 0.5 at z=0.** This is the mathematical foundation of the default decision threshold in logistic regression.\n",
    "4. **Distance metric choice matters** in KNN, especially with outliers. Manhattan distance is more forgiving because it uses absolute values instead of squares.\n",
    "5. **Model interpretability is a first-class requirement**, not a bonus. In regulated fields (healthcare, finance), the \"best\" model is the one stakeholders can understand and trust.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- This notebook uses small, clean datasets for demonstration. Real-world data is messier.\n",
    "- Decision tree interpretability decreases as depth increases — a tree with depth=20 is NOT interpretable.\n",
    "- The distance metric comparison depends on the specific dataset and outlier distribution.\n",
    "- Model choice involves many more factors (training time, scalability, feature types) beyond just interpretability.\n",
    "\n",
    "## What Could Be Improved / Next Steps\n",
    "\n",
    "- Practice computing sigmoid values **by hand** until z=0 → σ=0.5 is automatic\n",
    "- Study **linearization techniques** beyond polynomial features (log transforms, Box-Cox)\n",
    "- Explore **SHAP values** for making even KNN and complex models more interpretable\n",
    "- Try **GridSearchCV** comparing Euclidean vs. Manhattan on a dataset with real outliers\n",
    "- Review the **precision-recall tradeoff** for different decision thresholds (connected to sigmoid understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection (Required)\n",
    "\n",
    "**Answer briefly (3-6 sentences):**\n",
    "\n",
    "**1. Which 1-2 concepts were most challenging, and why?**\n",
    "\n",
    "The sigmoid function computation and residual plot interpretation were the most challenging. For the sigmoid, I confused the output at z=0 — I need to remember that e^0 = 1, making the denominator 2 and the output exactly 0.5. For residuals, I incorrectly associated \"random scatter\" with overfitting, when it's actually the opposite — random scatter means the model is working well and only noise remains.\n",
    "\n",
    "**2. What trade-offs or assumptions did you overlook in the interview?**\n",
    "\n",
    "I overlooked the importance of model interpretability when choosing between algorithms. I focused on algorithmic properties (non-parametric, small dataset suitability) rather than the audience's needs. I also assumed outlier removal is always the first response, when choosing a robust distance metric is often the better approach — especially in medical domains where extreme values carry clinical meaning.\n",
    "\n",
    "**3. What is your plan to improve over the next week?**\n",
    "\n",
    "I plan to: (1) practice computing the sigmoid function by hand for various z values until it's second nature, (2) review residual plot patterns by generating different datasets and predicting what the residuals will look like before plotting, (3) create a personal cheat sheet comparing R² vs. MSE vs. RMSE vs. MAE with concrete examples, and (4) build a decision framework for choosing models based on interpretability requirements vs. pure performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated by **Claude Opus 4.6** based on Muthuraj's interview quiz performance on 2026-02-27. It targets the 5 topics missed during the 15-question ML interview quiz (Score: 10/15, 67%).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
