{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# üß†üìö Job Interview Guide Workshop\n",
    "\n",
    "![Image Description](./images/OnlineJobInterview2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "\n",
    "## üéØ Learning Objectives\n",
    "- Practice **AI-mediated interview preparation** with a mix of *technical* and *behavioral/scenario-based* questions.\n",
    "- Reinforce core ML topics from the course through **targeted exercises**.\n",
    "- Produce a **graded, personalized study notebook** tailored to quiz results.\n",
    "- Demonstrate professionalism with **clear documentation** and **version control** (GitHub).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "\n",
    "## üìò Topics Covered (Review Scope)\n",
    "1. **Supervised vs. Unsupervised** learning algorithms  \n",
    "2. **Dependent vs. Independent Variables**  \n",
    "3. **Train / Validation / Test Split** (data leakage, stratification)  \n",
    "4. **Linear Regression**: residuals, linearization  \n",
    "5. **Regression Analysis**: parametric vs. non-parametric, **R¬≤**, **MSE**  \n",
    "6. **Logistic Regression**: intercept, slope, **cross-entropy**  \n",
    "7. **K-Nearest Neighbors (KNN)**: hyperparameters  \n",
    "8. **Decision Trees**: leaf nodes & predictions  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7",
   "metadata": {},
   "source": [
    "\n",
    "## üß≠ Workflow\n",
    "1. Collect workshop `.ipynb` files and study guide ‚Üí create `StudyGuide.txt` and `StudyMaterials.zip`\n",
    "2. Open a new LLM session and upload both files\n",
    "3. Paste the interview prompt ‚Üí complete the 15-question quiz\n",
    "4. Record results ‚Üí complete exercises ‚Üí push to GitHub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "\n",
    "## üìù Copy-Paste Prompt for Your LLM Session\n",
    "\n",
    "```text\n",
    "You are a seasoned Data Scientist, Machine Learning Engineer, and technical interviewer.\n",
    "I am a Data Scientist and ML Engineer, fresh out of college. You will interview me for an ML Specialist role.\n",
    "\n",
    "1) Unzip and read StudyMaterials.zip. Produce a 500-word summary of the ML learning content and coding patterns.\n",
    "2) Read StudyGuide.txt. Produce a 100-word summary of interview topics emphasized.\n",
    "3) Match study guide topics to workshop materials. Create a table listing each topic, coverage, and gaps.\n",
    "4) Create 15 multiple-choice questions (A-E) spanning all topics. Ask one at a time. Score me after all 15.\n",
    "5) Based on wrong answers, generate a personalized JobInterviewGuide_Workshop.ipynb with targeted exercises.\n",
    "Stop here and wait for my command to start the quiz.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i9",
   "metadata": {},
   "source": [
    "\n",
    "## üßÆ Grading Rubric\n",
    "| Component | Description | Weight |\n",
    "|---|---|---|\n",
    "| **Quiz Performance** | Score across 15 technical + scenario/behavioral questions | **40%** |\n",
    "| **Generated Study Notebook Quality** | Accuracy, completeness, clarity, and relevance | **40%** |\n",
    "| **Reflection** | Insightful, concise self-assessment | **20%** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## ü§ñ LLM Mock Interview Session ‚Äî 15-Question Quiz\n",
    "\n",
    "> Full question-and-answer transcript from the LLM interview session conducted using **Claude (Anthropic)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k1",
   "metadata": {},
   "source": [
    "\n",
    "### Quiz Transcript\n",
    "\n",
    "---\n",
    "\n",
    "**Q1. [Supervised Learning]**  \n",
    "A company wants to build a model to predict next month's sales revenue from historical data. Which type of learning best describes this task?\n",
    "\n",
    "A) Reinforcement learning  \n",
    "B) Unsupervised clustering  \n",
    "C) Supervised regression  \n",
    "D) Dimensionality reduction  \n",
    "E) Semi-supervised classification  \n",
    "\n",
    "**Answer given: C ‚úÖ CORRECT**  \n",
    "*Explanation: Predicting a continuous numeric output (sales revenue) from labeled historical data is a supervised regression task. The target variable is known and continuous.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. [Unsupervised Learning]**  \n",
    "Which algorithm is an example of unsupervised learning?\n",
    "\n",
    "A) Decision Tree  \n",
    "B) K-Nearest Neighbors  \n",
    "C) Linear Regression  \n",
    "D) K-Means Clustering  \n",
    "E) Logistic Regression  \n",
    "\n",
    "**Answer given: D ‚úÖ CORRECT**  \n",
    "*Explanation: K-Means discovers natural groupings in data without any labeled output. All other options are supervised ‚Äî they require a labeled target variable during training.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. [Independent vs. Dependent Variables]**  \n",
    "You are building a model to predict blood pressure using a patient's age, weight, and salt intake. Which of the following is an independent variable?\n",
    "\n",
    "A) Blood pressure  \n",
    "B) Patient outcome  \n",
    "C) Salt intake  \n",
    "D) Hospital admission rate  \n",
    "E) Model accuracy  \n",
    "\n",
    "**Answer given: C ‚úÖ CORRECT**  \n",
    "*Explanation: Independent variables (features/inputs) are what we feed into the model. Blood pressure is the dependent variable (target). Salt intake, age, and weight are independent variables.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. [Train/Val/Test Split]**  \n",
    "You have 10,000 samples and want a 70/15/15 train/val/test split. What is the correct approach?\n",
    "\n",
    "A) Split once into 70% train and 30% test; use train for validation  \n",
    "B) Split into 70% train and 30% temp; then split temp 50/50 into val and test  \n",
    "C) Use all data for training and evaluate with cross-validation only  \n",
    "D) Randomly shuffle and take every 3rd sample for each set  \n",
    "E) Use the first 70% of rows for train and last 30% for testing  \n",
    "\n",
    "**Answer given: B ‚úÖ CORRECT**  \n",
    "*Explanation: The two-step split is the correct method: first split off 30% as a temporary holdout, then split that 50/50 to yield 15% val and 15% test. This ensures no data contamination.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. [Data Leakage ‚Äî Target Leakage]**  \n",
    "You include a feature `days_since_last_payment` in a loan default prediction model. After deployment the model performs much worse than in testing. What likely happened?\n",
    "\n",
    "A) The model had too many features  \n",
    "B) The test set was too small  \n",
    "C) The feature is a result of the default event ‚Äî it is only available after the label is known  \n",
    "D) The model overfitted to noise  \n",
    "E) K was set too high in KNN  \n",
    "\n",
    "**Answer given: D ‚ùå INCORRECT** *(Correct answer: C)*  \n",
    "*Explanation: This is target leakage ‚Äî the feature `days_since_last_payment` is causally downstream of the default event itself. During training the model learned a shortcut that does not exist at prediction time, producing inflated test performance and collapsed deployment performance.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. [Linear Regression ‚Äî MSE & R¬≤]**  \n",
    "Two linear regression models are trained on the same dataset. Model A has R¬≤ = 0.88, MSE = 12.4. Model B has R¬≤ = 0.76, MSE = 24.1. Which model should you prefer and why?\n",
    "\n",
    "A) Model B ‚Äî lower R¬≤ means less overfitting  \n",
    "B) Model A ‚Äî higher R¬≤ and lower MSE indicate better fit and smaller prediction errors  \n",
    "C) Model B ‚Äî higher MSE means the model is more conservative  \n",
    "D) They are equivalent ‚Äî R¬≤ and MSE always agree  \n",
    "E) Model A ‚Äî but only if the dataset has fewer than 1,000 samples  \n",
    "\n",
    "**Answer given: B ‚úÖ CORRECT**  \n",
    "*Explanation: R¬≤ closer to 1 means more variance explained; lower MSE means smaller squared errors. Model A is strictly better on both metrics ‚Äî prefer it.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. [Residuals]**  \n",
    "After fitting a linear regression, you plot residuals vs. fitted values and notice the spread of residuals increases as fitted values increase (funnel shape). This indicates:\n",
    "\n",
    "A) The model is underfitting  \n",
    "B) There are too many features  \n",
    "C) Heteroscedasticity ‚Äî the variance of errors is not constant  \n",
    "D) The target variable is normally distributed  \n",
    "E) The model is perfectly calibrated  \n",
    "\n",
    "**Answer given: C ‚úÖ CORRECT**  \n",
    "*Explanation: A funnel-shaped residual plot is the signature of heteroscedasticity ‚Äî the OLS assumption of constant error variance (homoscedasticity) is violated. Fix: log-transform the target or use weighted least squares.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. [Logistic Regression ‚Äî Intercept]**  \n",
    "In a logistic regression model, the intercept Œ≤‚ÇÄ represents:\n",
    "\n",
    "A) The slope of the decision boundary  \n",
    "B) The model's accuracy when all features equal zero  \n",
    "C) The log-odds of the positive class when all independent variables equal zero  \n",
    "D) The total variance explained by the model  \n",
    "E) The cross-entropy loss at convergence  \n",
    "\n",
    "**Answer given: B ‚ùå INCORRECT** *(Correct answer: C)*  \n",
    "*Explanation: Œ≤‚ÇÄ is the log-odds (not probability or accuracy) of the positive class when every feature is zero. To convert to probability: P = sigmoid(Œ≤‚ÇÄ) = 1/(1+exp(-Œ≤‚ÇÄ)). Accuracy is a post-hoc classification metric, not part of the model equation.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q9. [Logistic Regression ‚Äî Cross-Entropy]**  \n",
    "Cross-entropy loss for binary classification is computed as:\n",
    "\n",
    "A) Sum of squared differences between predicted and actual values  \n",
    "B) Mean absolute error between probabilities and labels  \n",
    "C) ‚àí[y¬∑log(p) + (1‚àíy)¬∑log(1‚àíp)] averaged over all samples  \n",
    "D) R¬≤ subtracted from 1  \n",
    "E) The number of misclassified samples divided by total samples  \n",
    "\n",
    "**Answer given: C ‚úÖ CORRECT**  \n",
    "*Explanation: Binary cross-entropy (log loss) formula is ‚àí[y¬∑log(p) + (1‚àíy)¬∑log(1‚àíp)]. When y=1, only ‚àílog(p) matters. When y=0, only ‚àílog(1‚àíp) matters. This heavily penalizes confident wrong predictions.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q10. [KNN ‚Äî Effect of k]**  \n",
    "You train a KNN classifier with k=1 and observe 100% training accuracy but 61% test accuracy. What is the most likely cause?\n",
    "\n",
    "A) The features are not normalized  \n",
    "B) The dataset is too large for KNN  \n",
    "C) k=1 causes the model to overfit ‚Äî it memorizes each training point  \n",
    "D) The distance metric is incorrect  \n",
    "E) The class labels are imbalanced  \n",
    "\n",
    "**Answer given: C ‚úÖ CORRECT**  \n",
    "*Explanation: With k=1, every training point is its own nearest neighbor, guaranteeing 100% training accuracy. The decision boundary is jagged and overfits noise, causing poor generalization.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q11. [KNN ‚Äî Feature Scaling]**  \n",
    "Why must features be standardized or normalized before applying KNN?\n",
    "\n",
    "A) KNN cannot handle categorical variables without scaling  \n",
    "B) Scaling reduces the number of neighbors to compute  \n",
    "C) Features with larger numeric ranges will dominate distance calculations, biasing the model  \n",
    "D) Scaling converts the problem from regression to classification  \n",
    "E) KNN requires all features to be binary  \n",
    "\n",
    "**Answer given: C ‚úÖ CORRECT**  \n",
    "*Explanation: A feature ranging 0‚Äì10,000 (e.g., income) will overwhelm one ranging 0‚Äì1 (e.g., age normalized) in Euclidean distance, making the latter nearly irrelevant. Scaling equalizes feature contributions.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q12. [Decision Trees ‚Äî Splitting Criteria]**  \n",
    "A decision tree uses Gini impurity to decide splits. A node has Gini = 0.0. What does this mean?\n",
    "\n",
    "A) The node splits the data into equal halves  \n",
    "B) The node is the root of the tree  \n",
    "C) The node contains samples from all classes equally  \n",
    "D) The node is perfectly pure ‚Äî all samples belong to the same class  \n",
    "E) The tree has reached maximum depth  \n",
    "\n",
    "**Answer given: D ‚úÖ CORRECT**  \n",
    "*Explanation: Gini impurity = 0 means all samples at that node share the same class label ‚Äî perfect purity. The algorithm stops splitting at pure nodes (they become leaf nodes).*\n",
    "\n",
    "---\n",
    "\n",
    "**Q13. [Decision Trees ‚Äî Prediction]**  \n",
    "During inference, how does a decision tree classify a new sample?\n",
    "\n",
    "A) It averages predictions from all nodes  \n",
    "B) It applies a sigmoid to the root node output  \n",
    "C) It routes the sample through splits from root to a leaf, then outputs the leaf's majority class  \n",
    "D) It computes Euclidean distance to the nearest training sample  \n",
    "E) It calculates cross-entropy across all branches  \n",
    "\n",
    "**Answer given: C ‚úÖ CORRECT**  \n",
    "*Explanation: Prediction traverses the tree top-down: at each internal node, the sample takes the branch that satisfies the split condition. When it reaches a leaf node, the leaf outputs its stored majority class label.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q14. [Scenario ‚Äî Choosing Between Models]**  \n",
    "A hospital needs to predict patient readmission risk. The model must have both high recall (catch most at-risk patients) and explainability for medical staff. Which approach is best?\n",
    "\n",
    "A) Deep neural network ‚Äî highest accuracy  \n",
    "B) KNN ‚Äî straightforward prediction logic  \n",
    "C) Random Forest with SHAP values for explainability  \n",
    "D) Logistic Regression ‚Äî interpretable coefficients and probability outputs  \n",
    "E) K-Means clustering ‚Äî groups high-risk patients automatically  \n",
    "\n",
    "**Answer given: A ‚ùå INCORRECT** *(Correct answer: D)*  \n",
    "*Explanation: Logistic regression provides probability estimates (enabling a low threshold for high recall), directly interpretable coefficients that clinicians can reason about, and regulatory compliance in medical settings. Deep neural networks are opaque and not suitable when explainability is required. SHAP + Random Forest is a reasonable runner-up, but logistic regression is the safest and most practical first choice here.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q15. [Scenario ‚Äî Handling Overfitting]**  \n",
    "Your decision tree achieves 99% training accuracy but only 65% on the validation set. Which TWO actions would most effectively address this? (Choose the best single answer among combined options.)\n",
    "\n",
    "A) Increase the number of features and remove the validation set  \n",
    "B) Set `max_depth` and `min_samples_leaf` to constrain tree complexity  \n",
    "C) Use a larger learning rate and add more trees  \n",
    "D) Replace the decision tree with linear regression  \n",
    "E) Reduce the training set size to 50%  \n",
    "\n",
    "**Answer given: B ‚úÖ CORRECT**  \n",
    "*Explanation: `max_depth` limits how deep the tree grows (prevents memorization). `min_samples_leaf` requires a minimum number of samples at each leaf (avoids splits on noise). Together they are the two most direct regularization tools for decision trees.*\n",
    "\n",
    "---\n",
    "\n",
    "### üèÜ Final Score: **12 / 15 (80%)**\n",
    "\n",
    "| # | Topic | Result |\n",
    "|---|---|---|\n",
    "| 1 | Supervised vs. Unsupervised (regression) | ‚úÖ |\n",
    "| 2 | Unsupervised Learning (K-Means) | ‚úÖ |\n",
    "| 3 | Independent vs. Dependent Variables | ‚úÖ |\n",
    "| 4 | Train / Val / Test Split procedure | ‚úÖ |\n",
    "| 5 | Data Leakage ‚Äî Target Leakage | ‚ùå |\n",
    "| 6 | R¬≤ and MSE comparison | ‚úÖ |\n",
    "| 7 | Residuals ‚Äî Heteroscedasticity | ‚úÖ |\n",
    "| 8 | Logistic Regression Intercept (Œ≤‚ÇÄ) | ‚ùå |\n",
    "| 9 | Cross-Entropy Formula | ‚úÖ |\n",
    "| 10 | KNN ‚Äî k=1 Overfitting | ‚úÖ |\n",
    "| 11 | KNN ‚Äî Feature Scaling | ‚úÖ |\n",
    "| 12 | Decision Tree ‚Äî Gini Impurity | ‚úÖ |\n",
    "| 13 | Decision Tree ‚Äî Prediction Traversal | ‚úÖ |\n",
    "| 14 | Scenario ‚Äî Model Selection (Hospital) | ‚ùå |\n",
    "| 15 | Scenario ‚Äî Overfitting Decision Tree | ‚úÖ |\n",
    "\n",
    "**Topics to review:** Target Leakage (Q5), Logistic Regression Intercept Œ≤‚ÇÄ (Q8), Model Selection for Explainability (Q14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Record Your Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j0k1l2m3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quiz_results = {\n",
    "    \"name\": \"Kevinkumar Patel\",\n",
    "    \"date\": \"2026-02-27\",\n",
    "    \"model_used\": \"Claude (Anthropic)\",\n",
    "    \"overall_score\": 80,\n",
    "    \"num_correct\": 12,\n",
    "    \"topics_missed\": [\n",
    "        \"Q5  - Target Leakage: Diagnosed as overfitting instead of recognizing a causally downstream feature\",\n",
    "        \"Q8  - Logistic Regression Intercept: Confused log-odds with accuracy/probability when all features = 0\",\n",
    "        \"Q14 - Model Selection: Chose deep neural network over logistic regression for an explainability-required task\"\n",
    "    ],\n",
    "    \"behavioral_notes\": (\n",
    "        \"Good systematic reasoning on split procedure (Q4) and tree regularization (Q15). \"\n",
    "        \"Struggled to identify subtle forms of data leakage and to apply explainability constraints \"\n",
    "        \"when selecting models in clinical scenarios.\"\n",
    "    ),\n",
    "    \"next_steps_from_llm\": (\n",
    "        \"1) Study target leakage with concrete examples: features derived from or caused by the label. \"\n",
    "        \"2) Review logistic regression math from scratch: intercept is log-odds at X=0, convert via sigmoid. \"\n",
    "        \"3) Build a model selection decision framework: when accuracy vs explainability vs recall trade-offs apply. \"\n",
    "        \"4) Practice 2 scenario-based questions daily focusing on stakeholder constraints.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print('Quiz results recorded.')\n",
    "print(f\"Score: {quiz_results['num_correct']}/15 ({quiz_results['overall_score']}%)\")\n",
    "print(f\"Topics missed ({len(quiz_results['topics_missed'])}):\")  \n",
    "for t in quiz_results['topics_missed']:\n",
    "    print(f\"  ‚Ä¢ {t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "\n",
    "## üóÇ Coverage vs. Gaps (Generated by LLM)\n",
    "\n",
    "| Topic | Covered in Workshop Materials | Notes / Gaps |\n",
    "|---|---|---|\n",
    "| Supervised Learning (regression & classification) | ‚úÖ Yes | Examples in LinearRegressionArchitecture and LogisticRegressionClassifier |\n",
    "| Unsupervised Learning (clustering) | ‚ùå Gap | K-Means or clustering not present in any workshop notebook |\n",
    "| Dependent vs. Independent Variables | ‚úÖ Yes | Clearly defined in regression workshops |\n",
    "| Train / Val / Test Split Procedure | ‚úÖ Yes | Implemented in PerformanceMetricsClassification |\n",
    "| Target Leakage | ‚ùå Gap | Only preprocessing leakage is hinted at; feature-level (target) leakage not addressed |\n",
    "| Data Leakage via Preprocessing | ‚ö†Ô∏è Partial | Splitting before scaling is shown but not highlighted as an explicit lesson |\n",
    "| R¬≤ and MSE Interpretation | ‚úÖ Yes | Computed and discussed in LinearRegressionArchitecture_Workshop |\n",
    "| Heteroscedasticity / Residual Diagnostics | ‚ö†Ô∏è Partial | Residuals are plotted but heteroscedasticity is not named or diagnosed |\n",
    "| Logistic Regression ‚Äî Intercept (Œ≤‚ÇÄ) | ‚ö†Ô∏è Partial | Intercept printed in output but its meaning in log-odds terms is not explained |\n",
    "| Logistic Regression ‚Äî Cross-Entropy Formula | ‚úÖ Yes | log_loss used and reported in LogisticRegressionClassifier |\n",
    "| KNN Hyperparameters (k, distance, scaling) | ‚úÖ Yes | Fully explored in KNearestNeighbors_Workshop |\n",
    "| Decision Tree ‚Äî Gini Impurity & Splitting | ‚ö†Ô∏è Partial | Tree is built and evaluated but splitting criteria not explained |\n",
    "| Decision Tree ‚Äî Prediction Traversal | ‚úÖ Yes | plot_tree and predictions demonstrated |\n",
    "| Model Selection Trade-offs (explainability, recall) | ‚ùå Gap | Models are evaluated in isolation; no comparative decision framework |\n",
    "| Scenario-Based / Behavioral Questions | ‚ùå Gap | No interview simulation or scenario practice in the materials |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l2m3n4o5",
   "metadata": {},
   "source": [
    "\n",
    "## üß™ Targeted Practice\n",
    "\n",
    "### 1) Train / Validation / Test Split & Target Leakage üî™\n",
    "\n",
    "Implement a correct stratified split and demonstrate **target leakage** ‚Äî where a feature that is causally caused by the label inflates training performance and collapses at deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 2000\n",
    "\n",
    "# Simulate a loan default dataset\n",
    "income        = np.random.normal(50000, 15000, n).clip(10000, 120000)\n",
    "credit_score  = np.random.randint(300, 850, n)\n",
    "loan_amount   = np.random.normal(15000, 5000, n).clip(1000, 50000)\n",
    "default       = ((income < 35000) | (credit_score < 500)).astype(int)\n",
    "default       = np.where(np.random.rand(n) < 0.05, 1 - default, default)  # 5% noise\n",
    "\n",
    "# Target-leaking feature: days_since_last_payment is only known AFTER default occurs\n",
    "days_since_last_payment = np.where(default == 1,\n",
    "                                   np.random.randint(60, 180, n),   # defaulters miss payments\n",
    "                                   np.random.randint(1,  30,  n))   # non-defaulters pay on time\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'income': income,\n",
    "    'credit_score': credit_score,\n",
    "    'loan_amount': loan_amount,\n",
    "    'days_since_last_payment': days_since_last_payment,  # ‚Üê LEAKING FEATURE\n",
    "    'default': default\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {df.shape} | Default rate: {df['default'].mean():.2%}\")\n",
    "\n",
    "# ‚îÄ‚îÄ Stratified 70 / 15 / 15 split ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "features_safe  = ['income', 'credit_score', 'loan_amount']\n",
    "features_leaky = ['income', 'credit_score', 'loan_amount', 'days_since_last_payment']\n",
    "y = df['default']\n",
    "\n",
    "def make_splits(df, feature_cols):\n",
    "    X = df[feature_cols]\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(X, y, test_size=0.30,\n",
    "                                                 stratify=y, random_state=42)\n",
    "    X_val, X_te, y_val, y_te = train_test_split(X_tmp, y_tmp, test_size=0.50,\n",
    "                                                 stratify=y_tmp, random_state=42)\n",
    "    return X_tr, X_val, X_te, y_tr, y_val, y_te\n",
    "\n",
    "# ‚úÖ Correct model: no leaking feature\n",
    "X_tr, X_val, X_te, y_tr, y_val, y_te = make_splits(df, features_safe)\n",
    "model_safe = GradientBoostingClassifier(random_state=42).fit(X_tr, y_tr)\n",
    "print(f\"\\n‚úÖ Safe features only:\")\n",
    "print(f\"   Train acc: {accuracy_score(y_tr,  model_safe.predict(X_tr)) :.4f}\")\n",
    "print(f\"   Val   acc: {accuracy_score(y_val, model_safe.predict(X_val)):.4f}\")\n",
    "print(f\"   Test  acc: {accuracy_score(y_te,  model_safe.predict(X_te)) :.4f}\")\n",
    "\n",
    "# ‚ùå Leaky model: includes days_since_last_payment\n",
    "X_tr2, X_val2, X_te2, y_tr2, y_val2, y_te2 = make_splits(df, features_leaky)\n",
    "model_leaky = GradientBoostingClassifier(random_state=42).fit(X_tr2, y_tr2)\n",
    "print(f\"\\n‚ùå With TARGET-LEAKING feature (days_since_last_payment):\")\n",
    "print(f\"   Train acc: {accuracy_score(y_tr2,  model_leaky.predict(X_tr2)) :.4f}  ‚Üê inflated!\")\n",
    "print(f\"   Val   acc: {accuracy_score(y_val2, model_leaky.predict(X_val2)):.4f}  ‚Üê inflated!\")\n",
    "print(f\"   Test  acc: {accuracy_score(y_te2,  model_leaky.predict(X_te2)) :.4f}  ‚Üê inflated!\")\n",
    "print(\"\\n‚ö†Ô∏è  In production, days_since_last_payment is not yet known at prediction time.\")\n",
    "print(\"   The model would fail silently. This is target leakage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4o5p6q7",
   "metadata": {},
   "source": [
    "\n",
    "### 2) Linear Regression ‚Äî Residuals & Heteroscedasticity üìä\n",
    "\n",
    "Fit linear regression on a dataset with heteroscedastic errors. Diagnose using residual plots and apply a **log-transform** to fix the violation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o5p6q7r8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# Generate heteroscedastic data: variance grows with X\n",
    "X_raw = np.linspace(1, 10, 400)\n",
    "noise  = np.random.normal(0, X_raw * 1.5)   # variance ‚àù X  ‚Üí heteroscedastic\n",
    "y_raw  = 3.5 * X_raw + noise\n",
    "X_2d   = X_raw.reshape(-1, 1)\n",
    "\n",
    "# ‚îÄ‚îÄ Model A: Linear on raw data (violated assumption) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "linA = LinearRegression().fit(X_2d, y_raw)\n",
    "predA = linA.predict(X_2d)\n",
    "residA = y_raw - predA\n",
    "\n",
    "# ‚îÄ‚îÄ Model B: Linear on log(y) ‚Äî fixes heteroscedasticity ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "y_log  = np.log(y_raw.clip(min=0.01))\n",
    "linB   = LinearRegression().fit(X_2d, y_log)\n",
    "predB  = linB.predict(X_2d)\n",
    "residB = y_log - predB\n",
    "\n",
    "print(\"Model A (raw y):\")\n",
    "print(f\"  R¬≤ = {r2_score(y_raw, predA):.4f} | MSE = {mean_squared_error(y_raw, predA):.4f}\")\n",
    "print(f\"  Residual std: {residA.std():.4f}\")\n",
    "print(\"\\nModel B (log(y)):\")\n",
    "print(f\"  R¬≤ = {r2_score(y_log, predB):.4f} | MSE = {mean_squared_error(y_log, predB):.4f}\")\n",
    "print(f\"  Residual std: {residB.std():.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 8))\n",
    "\n",
    "# Raw fit\n",
    "axes[0, 0].scatter(X_raw, y_raw, alpha=0.4, s=15)\n",
    "axes[0, 0].plot(X_raw, predA, color='red', linewidth=2)\n",
    "axes[0, 0].set(title='Model A: Raw y', xlabel='X', ylabel='y')\n",
    "\n",
    "# Raw residuals ‚Äî funnel shape shows heteroscedasticity\n",
    "axes[0, 1].scatter(predA, residA, alpha=0.4, s=15)\n",
    "axes[0, 1].axhline(0, color='red', linestyle='--')\n",
    "axes[0, 1].set(title='Model A Residuals\\n(Funnel = Heteroscedasticity ‚ùå)', xlabel='Fitted', ylabel='Residual')\n",
    "\n",
    "# Log-transformed fit\n",
    "axes[1, 0].scatter(X_raw, y_log, alpha=0.4, s=15)\n",
    "axes[1, 0].plot(X_raw, predB, color='green', linewidth=2)\n",
    "axes[1, 0].set(title='Model B: log(y)', xlabel='X', ylabel='log(y)')\n",
    "\n",
    "# Log residuals ‚Äî random scatter confirms fix\n",
    "axes[1, 1].scatter(predB, residB, alpha=0.4, s=15)\n",
    "axes[1, 1].axhline(0, color='green', linestyle='--')\n",
    "axes[1, 1].set(title='Model B Residuals\\n(Random scatter = Fixed ‚úÖ)', xlabel='Fitted', ylabel='Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nüí° Log-transforming the target stabilizes variance and restores OLS assumptions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q7r8s9",
   "metadata": {},
   "source": [
    "\n",
    "### 3) Logistic Regression ‚Äî Intercept, Cross-Entropy & Model Selection üìà\n",
    "\n",
    "Interpret Œ≤‚ÇÄ, compare logistic regression vs. a black-box model using cross-entropy and explainability metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7r8s9t0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, accuracy_score, recall_score, roc_auc_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "feat_names = X.columns.tolist()\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\n",
    "\n",
    "# ‚îÄ‚îÄ Logistic Regression ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "lr_pipe = Pipeline([('sc', StandardScaler()), ('lr', LogisticRegression(max_iter=2000))])\n",
    "lr_pipe.fit(X_tr, y_tr)\n",
    "lr_proba = lr_pipe.predict_proba(X_te)[:, 1]\n",
    "lr_pred  = lr_pipe.predict(X_te)\n",
    "\n",
    "lr_model = lr_pipe.named_steps['lr']\n",
    "beta0    = lr_model.intercept_[0]\n",
    "p_at_zero = 1 / (1 + np.exp(-beta0))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  LOGISTIC REGRESSION ‚Äî Intercept Interpretation (Q8 review)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Œ≤‚ÇÄ (intercept) = {beta0:.4f}\")\n",
    "print(f\"  Interpretation: when ALL features = 0 (after scaling),\")\n",
    "print(f\"  the log-odds of the positive class = {beta0:.4f}\")\n",
    "print(f\"  ‚Üí P(y=1 | all features = 0) = sigmoid({beta0:.4f}) = {p_at_zero:.4f}\")\n",
    "print(f\"\\n  ‚ö†Ô∏è  Œ≤‚ÇÄ is NOT the accuracy. It is the log-odds baseline.\")\n",
    "\n",
    "# ‚îÄ‚îÄ Random Forest (for comparison) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_tr, y_tr)\n",
    "rf_proba = rf.predict_proba(X_te)[:, 1]\n",
    "rf_pred  = rf.predict(X_te)\n",
    "\n",
    "# ‚îÄ‚îÄ Side-by-side comparison ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Recall (sensitivity)', 'AUC-ROC', 'Cross-Entropy', 'Explainable?'],\n",
    "    'Logistic Regression': [\n",
    "        f\"{accuracy_score(y_te, lr_pred):.4f}\",\n",
    "        f\"{recall_score(y_te, lr_pred):.4f}\",\n",
    "        f\"{roc_auc_score(y_te, lr_proba):.4f}\",\n",
    "        f\"{log_loss(y_te, lr_proba):.4f}\",\n",
    "        \"‚úÖ Yes ‚Äî coefficients map directly to log-odds\"\n",
    "    ],\n",
    "    'Random Forest': [\n",
    "        f\"{accuracy_score(y_te, rf_pred):.4f}\",\n",
    "        f\"{recall_score(y_te, rf_pred):.4f}\",\n",
    "        f\"{roc_auc_score(y_te, rf_proba):.4f}\",\n",
    "        f\"{log_loss(y_te, rf_proba):.4f}\",\n",
    "        \"‚ùå No ‚Äî requires SHAP/LIME for post-hoc explanation\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  MODEL COMPARISON (Q14 review)\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\nüí° When explainability is mandatory (medical, legal, financial):\")\n",
    "print(\"   Logistic Regression is preferred even if RF has slightly better accuracy.\")\n",
    "print(\"   Adjust the decision threshold to balance precision/recall for high-recall needs.\")\n",
    "\n",
    "# Show top 5 most impactful features for logistic regression\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': feat_names,\n",
    "    'coefficient': lr_model.coef_[0]\n",
    "}).reindex(lr_model.coef_[0].argsort()[::-1]).head(5)\n",
    "print(\"\\nTop 5 features by logistic regression coefficient (log-odds contribution):\")\n",
    "print(coef_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8s9t0u1",
   "metadata": {},
   "source": [
    "\n",
    "### 4) KNN & Decision Tree ‚Äî Full Pipeline with Hyperparameter Tuning üå≥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9t0u1v2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Use Wine dataset (3 classes, 13 features)\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=3)\n",
    "print(f\"Wine dataset ‚Äî Train: {len(X_tr)} | Test: {len(X_te)} | Classes: {np.unique(y)}\")\n",
    "\n",
    "# ‚îÄ‚îÄ KNN with cross-validation across k ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\nKNN cross-validation across k (5-fold):\")\n",
    "ks = range(1, 21)\n",
    "cv_means, cv_stds = [], []\n",
    "\n",
    "for k in ks:\n",
    "    pipe_knn = Pipeline([\n",
    "        ('sc',  StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=k, metric='minkowski', weights='uniform'))\n",
    "    ])\n",
    "    scores = cross_val_score(pipe_knn, X_tr, y_tr, cv=5, scoring='accuracy')\n",
    "    cv_means.append(scores.mean())\n",
    "    cv_stds.append(scores.std())\n",
    "\n",
    "best_k = list(ks)[np.argmax(cv_means)]\n",
    "print(f\"Best k = {best_k} | CV accuracy = {max(cv_means):.4f} ¬± {cv_stds[np.argmax(cv_means)]:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "best_knn = Pipeline([('sc', StandardScaler()),\n",
    "                     ('knn', KNeighborsClassifier(n_neighbors=best_k))])\n",
    "best_knn.fit(X_tr, y_tr)\n",
    "print(f\"Test accuracy (k={best_k}): {accuracy_score(y_te, best_knn.predict(X_te)):.4f}\")\n",
    "\n",
    "# Plot CV results\n",
    "cv_means_arr = np.array(cv_means)\n",
    "cv_stds_arr  = np.array(cv_stds)\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(list(ks), cv_means_arr, marker='o', linewidth=2, label='CV Mean Accuracy')\n",
    "plt.fill_between(list(ks), cv_means_arr - cv_stds_arr,\n",
    "                 cv_means_arr + cv_stds_arr, alpha=0.2, label='¬±1 std')\n",
    "plt.axvline(best_k, color='red', linestyle='--', label=f'Best k={best_k}')\n",
    "plt.xlabel('k'); plt.ylabel('CV Accuracy')\n",
    "plt.title('KNN: Cross-Validated Accuracy vs. k (Wine Dataset)')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ‚îÄ‚îÄ Decision Tree: depth vs. train/test accuracy ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\nDecision Tree ‚Äî Depth vs. Accuracy (Q15 review):\")\n",
    "rows = []\n",
    "for depth in [1, 2, 3, 4, 5, 8, None]:\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=2, random_state=0)\n",
    "    dt.fit(X_tr, y_tr)\n",
    "    rows.append({\n",
    "        'max_depth': str(depth), 'leaves': dt.get_n_leaves(),\n",
    "        'train_acc': round(dt.score(X_tr, y_tr), 4),\n",
    "        'test_acc':  round(dt.score(X_te, y_te),  4)\n",
    "    })\n",
    "print(pd.DataFrame(rows).to_string(index=False))\n",
    "print(\"\\nüí° max_depth=None ‚Üí overfitting. max_depth=3-4 gives best generalization here.\")\n",
    "\n",
    "# Visualize best depth\n",
    "dt_best = DecisionTreeClassifier(max_depth=3, min_samples_leaf=2, random_state=0)\n",
    "dt_best.fit(X_tr, y_tr)\n",
    "feature_names = load_wine().feature_names\n",
    "plt.figure(figsize=(14, 6))\n",
    "plot_tree(dt_best, feature_names=feature_names,\n",
    "          class_names=['class_0', 'class_1', 'class_2'],\n",
    "          filled=False, rounded=True, fontsize=8)\n",
    "plt.title(f'Decision Tree (max_depth=3, min_samples_leaf=2) ‚Äî Wine Dataset')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra_header",
   "metadata": {},
   "source": [
    "\n",
    "## üî¨ Extra Practice ‚Äî Reinforcing Missed Topics\n",
    "\n",
    "### Exercise A: Target Leakage Detection Checklist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra_leakage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Feature audit checklist ‚Äî ask these questions before training any model\n",
    "print(\"=\" * 65)\n",
    "print(\"  TARGET LEAKAGE DETECTION CHECKLIST\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "checklist = [\n",
    "    (\"Is the feature available BEFORE the prediction time?\",\n",
    "     \"days_since_last_payment ‚Äî only exists after default occurs ‚Üí ‚ùå LEAKY\"),\n",
    "    (\"Is the feature causally downstream of the label?\",\n",
    "     \"collection_calls_made ‚Äî only happens if default happens ‚Üí ‚ùå LEAKY\"),\n",
    "    (\"Does the feature have suspiciously high correlation with the label?\",\n",
    "     \"correlation > 0.95 between a feature and label is a red flag ‚Üí ‚ö†Ô∏è INVESTIGATE\"),\n",
    "    (\"Would this feature be available at inference time in production?\",\n",
    "     \"credit_score, income, loan_amount ‚Äî all known before lending ‚Üí ‚úÖ SAFE\"),\n",
    "    (\"Was this feature engineered from future data?\",\n",
    "     \"30-day rolling average computed using future timestamps ‚Üí ‚ùå LEAKY\"),\n",
    "]\n",
    "\n",
    "for i, (question, example) in enumerate(checklist, 1):\n",
    "    print(f\"\\n  {i}. {question}\")\n",
    "    print(f\"     Example: {example}\")\n",
    "\n",
    "print(\"\\nüí° Rule of thumb: if a feature would NOT exist at prediction time\")\n",
    "print(\"   in a production system, it must be excluded from training.\")\n",
    "\n",
    "# Show what extreme correlation looks like\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "default_labels = np.random.randint(0, 2, n)\n",
    "leaky_feature  = default_labels + np.random.normal(0, 0.05, n)\n",
    "safe_feature   = np.random.normal(50000, 15000, n)\n",
    "\n",
    "print(f\"\\nCorrelation with label:\")\n",
    "print(f\"  Leaky feature: {np.corrcoef(leaky_feature, default_labels)[0,1]:.4f} ‚Üê suspicious!\")\n",
    "print(f\"  Safe  feature: {np.corrcoef(safe_feature,  default_labels)[0,1]:.4f} ‚Üê normal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra_b_header",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise B: Logistic Regression Intercept ‚Äî Math Walkthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra_intercept",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Demonstrate what beta_0 controls: the default probability when all features = 0\n",
    "print(\"=\" * 60)\n",
    "print(\"  LOGISTIC REGRESSION INTERCEPT ‚Äî MATHEMATICAL WALKTHROUGH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "beta_0_values = [-3, -1, 0, 1, 3]\n",
    "beta_1 = 1.5\n",
    "\n",
    "print(\"\\nWhen X=0, only Œ≤‚ÇÄ matters:\")\n",
    "for b0 in beta_0_values:\n",
    "    log_odds_at_zero = b0\n",
    "    p_at_zero = 1 / (1 + np.exp(-b0))\n",
    "    print(f\"  Œ≤‚ÇÄ={b0:+.1f} ‚Üí log-odds={log_odds_at_zero:+.1f} ‚Üí P(y=1|X=0) = {p_at_zero:.4f}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Œ≤‚ÇÄ is NOT accuracy ‚Äî it shifts the sigmoid curve left/right.\")\n",
    "print(\"   Larger Œ≤‚ÇÄ = higher baseline probability (curve shifted left)\")\n",
    "print(\"   Smaller Œ≤‚ÇÄ = lower baseline probability (curve shifted right)\")\n",
    "\n",
    "# Visualize how Œ≤‚ÇÄ shifts the decision boundary\n",
    "X_range = np.linspace(-5, 5, 300)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "for b0 in beta_0_values:\n",
    "    prob = 1 / (1 + np.exp(-(b0 + beta_1 * X_range)))\n",
    "    axes[0].plot(X_range, prob, label=f'Œ≤‚ÇÄ={b0:+.0f}')\n",
    "axes[0].axhline(0.5, color='black', linestyle=':', linewidth=1)\n",
    "axes[0].set(xlabel='X', ylabel='P(y=1)', title=f'Effect of Œ≤‚ÇÄ on Decision Boundary (Œ≤‚ÇÅ={beta_1})')\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# Decision boundary shifts with Œ≤‚ÇÄ\n",
    "decision_boundaries = [-b0 / beta_1 for b0 in beta_0_values]\n",
    "axes[1].barh([f'Œ≤‚ÇÄ={b0:+.0f}' for b0 in beta_0_values], decision_boundaries)\n",
    "axes[1].axvline(0, color='gray', linestyle='--')\n",
    "axes[1].set(xlabel='Decision boundary (X where P=0.5)', title='Decision Boundary Location')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìå Summary:\")\n",
    "print(\"  Œ≤‚ÇÄ ‚Üí shifts where the sigmoid crosses 0.5 (the decision boundary)\")\n",
    "print(\"  Œ≤‚ÇÅ ‚Üí controls the steepness / how fast probability changes with X\")\n",
    "print(\"  Neither Œ≤‚ÇÄ nor Œ≤‚ÇÅ directly represents accuracy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional_checks",
   "metadata": {},
   "source": [
    "\n",
    "## üß∞ (Optional) Local Material Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local_checks",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, zipfile, textwrap\n",
    "\n",
    "for fname in ['StudyGuide.txt', 'StudyMaterials.zip']:\n",
    "    print(f'{fname}:', 'FOUND' if os.path.exists(fname) else 'NOT FOUND')\n",
    "\n",
    "if os.path.exists('StudyGuide.txt'):\n",
    "    with open('StudyGuide.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read(700)\n",
    "    print('\\n--- StudyGuide (first 700 chars) ---\\n')\n",
    "    print(textwrap.shorten(content, width=700))\n",
    "\n",
    "if os.path.exists('StudyMaterials.zip'):\n",
    "    with zipfile.ZipFile('StudyMaterials.zip', 'r') as z:\n",
    "        print('\\n--- ZIP Contents ---')\n",
    "        for info in z.infolist()[:30]:\n",
    "            print(info.filename, info.file_size, 'bytes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection",
   "metadata": {},
   "source": [
    "\n",
    "## ü™û Reflection\n",
    "\n",
    "**1. Which 1‚Äì2 concepts were most challenging, and why?**\n",
    "\n",
    "The most challenging concept was **target leakage** ‚Äî specifically, recognizing when a feature is causally *caused by* the label rather than being a cause *of* it. In Q5, I diagnosed the deployment failure as overfitting because the symptoms looked identical: good training performance, poor real-world performance. I missed the root cause, which was that `days_since_last_payment` only exists after a default occurs, making it impossible to observe at prediction time. The second challenging concept was the **logistic regression intercept Œ≤‚ÇÄ**: I incorrectly mapped it to accuracy rather than recognizing it as the log-odds baseline when all features equal zero, which is a subtly different quantity.\n",
    "\n",
    "**2. What trade-offs or assumptions did you overlook in the interview?**\n",
    "\n",
    "In Q14, I prioritized raw accuracy over explainability when selecting a model for a clinical setting. I overlooked the hard constraint that medical stakeholders legally and ethically require interpretable predictions ‚Äî a neural network would likely not pass hospital compliance review regardless of its accuracy. I also failed to consider that logistic regression's probability outputs allow clinicians to tune the classification threshold for high recall (catching most at-risk patients), making it the better fit for the stated requirements. The lesson is to identify hard constraints (regulation, explainability, latency) before optimizing for accuracy.\n",
    "\n",
    "**3. What is your plan to improve over the next week?**\n",
    "\n",
    "This week I will work through five real-world case studies of target leakage (credit risk, healthcare, e-commerce churn) and for each feature I will ask: *would this be available in production before the prediction is needed?* For logistic regression, I will write out the full math ‚Äî from linear combination to log-odds to sigmoid to probability ‚Äî by hand at least three times until Œ≤‚ÇÄ and Œ≤‚ÇÅ feel intuitive. For model selection, I will build a simple decision matrix with axes for accuracy, explainability, inference speed, and regulatory context, and practice applying it to at least two scenario questions per day until the framework is automatic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission",
   "metadata": {},
   "source": [
    "\n",
    "## üö¢ Submission (GitHub)\n",
    "- Ensure the notebook is **executed** and **saved** with your recorded results and completed exercises.\n",
    "- Add (in a new notebook called **JobInterviewLLMSession.ipynb**) the entire contents of the LLM session from start to end.\n",
    "- Push to your GitHub repository:\n",
    "\n",
    "```bash\n",
    "git add JobInterviewGuide_Workshop.ipynb \n",
    "git add JobInterviewLLMSession.ipynb\n",
    "git commit -m \"Add graded JobInterviewGuide_Workshop notebook and LLM session\"\n",
    "git push origin main\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
